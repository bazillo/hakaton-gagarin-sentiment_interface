{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Использование алгоритма Ахо-Корасик для определения сущностей"
   ],
   "metadata": {
    "id": "ZGnA3N6rWNSN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMzNyVapWAb_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_word_indices(texts: list[str], words: list[str], name_to_id: dict):\n",
    "    '''\n",
    "    Возвращает два массива: в первом для каждого сообщения перечислены issuer_id,\n",
    "    синонимы к которым были найдены, во втором перечислены сами синонимы\n",
    "    :param texts: массив сообщений\n",
    "    :param words: массив синонимов, вхождения которых будем искать в сообщениях\n",
    "    :param name_to_id: Словарь, где каждому синониму соответствует некоторый issuer_id\n",
    "    '''\n",
    "    # Инициализируем автомата для Ахо-Корасик\n",
    "    automaton = Automaton()\n",
    "\n",
    "    # Добавляем слова в автомат\n",
    "    for idx, word in enumerate(words):\n",
    "        automaton.add_word(word, idx)\n",
    "\n",
    "    # Строим автомат\n",
    "    automaton.make_automaton()\n",
    "\n",
    "    id_list = []\n",
    "    word_list = []\n",
    "    # Итерируемся по всем сообщениям\n",
    "    for text in texts:\n",
    "        # Мапим issuer_id к синониму, который встретился в сообщении\n",
    "        ids_found_in_text = set()\n",
    "        words_found_in_text = set()\n",
    "        # Находим вхождения словарных слов в сообщения и сохраняем их\n",
    "        for end_index, word_index in automaton.iter(text.lower()):\n",
    "            if name_to_id[words[word_index]] not in ids_found_in_text:\n",
    "                ids_found_in_text.add(name_to_id[words[word_index]])\n",
    "                words_found_in_text.add(words[word_index])\n",
    "\n",
    "        # Список issuer_id найденных в каждом сообщении\n",
    "        id_list.append(list(ids_found_in_text))\n",
    "        # Список самих синонимов, которые были найдены\n",
    "        word_list.append(list(words_found_in_text))\n",
    "\n",
    "    return id_list, word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Автогенерация синонимов"
   ],
   "metadata": {
    "id": "NLIahASpdUrY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def add_new_synonyms(synonyms):\n",
    "    \"\"\"Функция для добавляения новых синонимов по падежам и числам\"\"\"\n",
    "    new_synonyms = np.full((synonyms.shape[0], 12), fill_value='' , dtype=object)\n",
    "    # Падеж\n",
    "    word_case = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']\n",
    "    # Число\n",
    "    word_num = ['sing', 'plur']\n",
    "    for issuer_id in range(len(synonyms)):\n",
    "        if (len(synonyms[issuer_id]) <= 2):\n",
    "            whole_phrase = synonyms[issuer_id][0].split(sep=' ')\n",
    "        else:\n",
    "            whole_phrase = synonyms[issuer_id][2].split(sep=' ')\n",
    "        # Будем склонять каждое слово во фразе\n",
    "        for word in whole_phrase:\n",
    "            phrase = morph.parse(word)[0]\n",
    "            for i in range(12):\n",
    "                try:\n",
    "                    new_synonyms[issuer_id][i] += phrase.inflect({f'{word_num[(i < 6)]}', f'{word_case[i % 6]}'}).word + \" \"\n",
    "                except AttributeError:\n",
    "                    new_synonyms[issuer_id][i] += word + \" \"\n",
    "    return new_synonyms"
   ],
   "metadata": {
    "id": "zJ0S46KAdeJ_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Аугментация данных"
   ],
   "metadata": {
    "id": "cg8gNhE2hIHo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_synonym(word):\n",
    "  for i in synonyms:\n",
    "    if word in i:\n",
    "      return i[randint(0, len(i) - 1)]\n",
    "\n",
    "def get_syn_len(word):\n",
    "  for i in synonyms:\n",
    "    if word in i:\n",
    "      return len(i)\n",
    "  return 0\n",
    "\n",
    "\n",
    "def augmentate(df):\n",
    "  \"\"\"Расширяет датасет путём добавления новых примеров для классов с редкими лейблами\"\"\"\n",
    "  new_df = df.copy()\n",
    "  for index, row in oold_df.iterrows():\n",
    "      aspect = row['aspect']\n",
    "      label = row['label']\n",
    "      for _ in range(\n",
    "          min(weights[label], get_syn_len(aspect) - 5)\n",
    "          ):\n",
    "          new_row = row\n",
    "          new_row['aspect'] = get_synonym(aspect)\n",
    "          new_df.loc[len(new_df)] = new_row\n",
    "\n",
    "  return new_df"
   ],
   "metadata": {
    "id": "xqmzhZgWhMkU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Классы для работы с моделью"
   ],
   "metadata": {
    "id": "e7g3TMuBdFNy",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Определение архитектуры модели"
   ],
   "metadata": {
    "id": "GPgJe5dkeSg_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerClassificationModel(nn.Module):\n",
    "    \"\"\"Надстройка над предобученной LLM, добавление классификационной головы с несколькими линейными слоями\"\"\"\n",
    "\n",
    "    def __init__(self, base_transformer_model: str, num_classes: int, num_dense_layers: int):\n",
    "        super(TransformerClassificationModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(base_transformer_model)\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(base_transformer_model, config=config)\n",
    "\n",
    "        layers = []\n",
    "        input_size = self.backbone.config.hidden_size\n",
    "        for _ in range(num_dense_layers - 1):\n",
    "            layers.append(nn.Linear(input_size, input_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(input_size, num_classes))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        probabilities = self.softmax(logits)\n",
    "        return {'logits': logits, 'probabilities': probabilities, 'backbone outputs': outputs, }\n",
    "\n",
    "\n",
    "def freeze_backbone_function(model: TransformerClassificationModel, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_data(tokenizer, texts, aspects):\n",
    "    inputs = tokenizer([wrap(text, aspect) for text, aspect in zip(texts, aspects)],\n",
    "                            max_length=256, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "def evaluate(model, tokenizer, dataset, device):\n",
    "    model.eval()\n",
    "    test_texts, test_aspects = dataset[\"text\"], dataset[\"aspect\"]\n",
    "    test_inputs = preprocess_data(tokenizer, test_texts, test_aspects)\n",
    "    test_data = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask = batch\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "id": "xWM4XDgsXoDG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Класс для тренировки и вычисления предсказаний"
   ],
   "metadata": {
    "id": "US_D2euHeYK9",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerClassificationTrainer:\n",
    "    \"\"\"Класс, используемый для обучения и оценки модели классификации\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, max_length=MAX_LENGTH, batch_size=BATCH_SIZE, lr=LR,\n",
    "                 num_epochs=NUM_EPOCHS, freeze_backbone=FREEZE_BACKBONE):\n",
    "        self.model = freeze_backbone_function(model, freeze_backbone)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def preprocess_data(self, texts, aspects, labels):\n",
    "        #inputs = self.tokenizer(texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        inputs = self.tokenizer([wrap(text, aspect) for text, aspect in zip(texts, aspects)],\n",
    "                                max_length=self.max_length, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        labels = torch.tensor(labels)\n",
    "        return inputs, labels\n",
    "\n",
    "    def train(self, train_texts, train_aspects, train_labels):\n",
    "        train_inputs, train_labels = self.preprocess_data(train_texts, train_aspects, train_labels)\n",
    "        train_data = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, attention_mask, labels = batch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                logits = outputs['logits']\n",
    "                loss = loss_fn(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, test_texts, test_aspects, test_labels):\n",
    "        self.model.eval()\n",
    "        test_inputs, test_labels = self.preprocess_data(test_texts, test_aspects, test_labels)\n",
    "        test_data = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "        test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                logits = outputs['logits']\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                predictions.extend(preds)\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        f1 = f1_score(true_labels, predictions, average='macro')\n",
    "        print(f\"F1 score: {f1:.4f}\")\n",
    "\n",
    "def freeze_backbone_function(model: TransformerClassificationModel, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    return model"
   ],
   "metadata": {
    "id": "3lKwwmK_X5Q9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}